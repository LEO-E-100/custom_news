{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Custom News Feed\n",
    "\n",
    "Principle is to build an email service app that will deliver a daily email with 5 recommended articles based on what articles are in a persons pocket account (labelled dataset). This will initally mean creating a model based on data pulled from pocket using the API. Laterly this will mean creating a service that will pull news from RSS feeds and testing them against the model to find top recommendations. Then creating a second service that will send out automatic emails with recommended articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import config\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating supervised dataset\n",
    "\n",
    "Using the Pocket API to create the initial dataset. Having curated a dataset by adding ~200 articles to pocket and tagged them with 'y' or 'n' depending on whether the user is interested in the article. Ultimately this will give a dataframe of article urls and a label of whether the user is interested or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'code=1ffb9844-dd4d-17e3-00db-ae30f5'\n",
      "1ffb9844-dd4d-17e3-00db-ae30f5\n"
     ]
    }
   ],
   "source": [
    "auth_params = {\n",
    "    'consumer_key' : config.consumer_key, \n",
    "    'redirect_uri' : 'https://twitter.com/leojpedwards'\n",
    "}\n",
    "tkn = requests.post('https://getpocket.com/v3/oauth/request', data=auth_params)\n",
    "string_tkn = str(tkn.content)\n",
    "split_tkn = string_tkn.split('=')[1].replace('\\'', '')\n",
    "print(tkn.content)\n",
    "print(split_tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'403 Forbidden'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr_params = {\n",
    "    'consumer_key' : config.consumer_key, \n",
    "    'code' : '1ffb9844-dd4d-17e3-00db-ae30f5'\n",
    "}\n",
    "usr = requests.post('https://getpocket.com/v3/oauth/authorize', data=usr_params)\n",
    "usr.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_params = {\n",
    "    'consumer_key' : config.consumer_key,\n",
    "    'access_token' : config.access_token,\n",
    "    'tag' : 'n'\n",
    "}\n",
    "no_result = requests.post('https://getpocket.com/v3/get', data=no_params)\n",
    "no_result.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_jf = no_result.json()\n",
    "no_jd = no_jf['list']\n",
    "no_urls = []\n",
    "for i in no_jd.values():\n",
    "    no_urls.append(i.get('resolved_url'))\n",
    "no_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_uf = pd.DataFrame(no_urls, columns=['urls'])\n",
    "no_uf = no_uf.assign(wanted = lambda x: 'n')\n",
    "no_uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yes_params = {\n",
    "    'consumer_key' : config.consumer_key,\n",
    "    'access_token' : config.access_token,\n",
    "    'tag' : 'y'\n",
    "}\n",
    "yes_result = requests.post('https://getpocket.com/v3/get', data=yes_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_jf = yes_result.json()\n",
    "yes_jd = no_jf['list']\n",
    "yes_urls = []\n",
    "for i in yes_jd.values():\n",
    "    yes_urls.append(i.get('resolved_url'))\n",
    "yes_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_uf = pd.DataFrame(yes_urls, columns=['urls'])\n",
    "yes_uf = yes_uf.assign(wanted = lambda x: 'y')\n",
    "yes_uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([yes_uf, no_uf])\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping article content\n",
    "\n",
    "Once the URLs have been recovered it is necessary to scrape the text from these articles in order to carry out the NLP steps. This requires the use of scraping on a number of different web sources. This would be a time consuming process if I were to write a bespoke web scraper for each website. Therefore I decided to use an link embedding service with an api to query. Initially I tried to use embed.ly however this is now a very expensive paid service. I instead have used embed.rocks. This is tested below and applied to the whole list of article URLs from above. \n",
    "\n",
    "Once the raw HTML has been extracted from the API, it was necessary to add just the text as a new column which is done using BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test embed API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = requests.get('https://api.embed.rocks/api/?url=http://www.randalolson.com/2014/10/27/the-reddit-world-map/&key=' + config.embed_rocks_key)\n",
    "test_2 = json.loads(test.text)\n",
    "test_3 = test_2.get('article')\n",
    "test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "def get_html(x):\n",
    "    qurl = urllib.parse.quote(x)\n",
    "    rhtml = requests.get('https://api.embed.rocks/api/?url=' + qurl + '&key=' + config.embed_rocks_key)\n",
    "    try:\n",
    "        ctnt = json.loads(rhtml.text).get('article')\n",
    "    except ValueError:\n",
    "        ctnt = None\n",
    "    return ctnt\n",
    "df.loc[:, 'html'] = df['urls'].map(get_html)\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def get_text(x):\n",
    "    soup = BeautifulSoup(x, 'lxml')\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "df.loc[:, 'text'] = df['html'].map(get_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "From the text column it is possible to call a vectorizer in order to turn the text data into a usable matrix format for Machine Learning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(ngram_range=(1,3), stop_words='english', min_df=3)\n",
    "tv = vect.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Building a Support Vector Machine model from the vectorised data. This step will require some evaluation of the quality of the model which has not yet been done as initially it was thought that the iterative process would ensure that the model was effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "model = clf.fit(tv, df['wanted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Gspread credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "json_file = 'Custom News Feed-d7876b12a476.json'\n",
    "\n",
    "scope = ['https://spreadsheets.google.com/feeds']\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(json_file, scope)\n",
    "\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = gc.open('NewsFeed')\n",
    "sh = ws.sheet1\n",
    "zd = list(zip(sh.col_values(2), sh.col_values(3), sh.col_values(4)))\n",
    "zf = pd.DataFrame(zd, columns=['title', 'urls', 'html'])\n",
    "zf.replace('', pd.np.nan, inplace=True)\n",
    "zf.dropna(inplace=True)\n",
    "zf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf.loc[:, 'text'] = zf['html'].map(get_text) \n",
    "zf.reset_index(drop=True, inplace=True)\n",
    "test_matrix = vect.transform(zf['text'])\n",
    "test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(model.predict(test_matrix), columns = ['wanted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rez = pd.merge(results, zf, left_index=True, right_index=True)\n",
    "rez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hypothetical correction method\n",
    "change_to_no = [1, 7, 16]\n",
    "\n",
    "change_to_yes = [0, 9, 27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rez.iloc[change_to_yes].index:\n",
    "    rez.iloc[i]['wanted'] = 'y'\n",
    "for i in rez.iloc[change_to_no].index:\n",
    "    rez.iloc[i]['wanted'] = 'n'\n",
    "rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([df[['wanted', 'text']], rez[['wanted', 'text']]])\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rebuild model with new data\n",
    "tvcomb = vect.fit_transform(combined['text'], combined['wanted'])\n",
    "model = clf.fit(tvcomb, combined['wanted'])\n",
    "# Iterate this process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open(r'news_model_pickle.pkl', 'wb'))\n",
    "pickle.dump(vect, open(r'news_vect_pickle.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TODO\n",
    "\n",
    "- Add news article to supervised dataset\n",
    "- More articles in supervised dataset\n",
    "- Flask web app\n",
    "- Add config variables to config file\n",
    "- Score top articles rather than simple yes/no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
